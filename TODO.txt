
QKV/FFN micro‑FSMs (later, if you need area/timing)

    Q1. Time‑multiplex matrixVectorMult for Q/K/V and W_O
        One MAC lane accumulates over HeadDimension/ModelDim with a “done” pulse.
        Turn qkvDoneSig/ffnDoneSig from True into the real pulses.


Hazard/overlap (once TDP is in place)

    H1. Overlap reads and writes
        While port A streams reads for t = 0..pos, port B writes K,V for row pos.
        Keep the t==pos bypass in streamHeadAttention to avoid read‑during‑write issues.



Numeric realism

    Using Float throughout with sin/cos/exp/softmax will not synthesize efficiently. For a hardware decoder you’ll want:
        Fixed‑point or bfloat16 datapath.
        RoPE via LUT or CORDIC.
        Softmax via two‑pass streaming log‑sum‑exp or a stable single‑pass approximation.
        Dot products accumulated sequentially to keep area reasonable.
    Clash supports Float, but resource usage will be enormous unless you scale model sizes way down.



What to change at a high level

A workable 5‑stage per‑layer micro‑architecture (each stage can itself iterate):

    S1: Pre‑norm and Q/K/V
        x̂ = RMSNorm_att(x)
        Q/K/V matmuls for the heads needed at this layer (MQA: compute K,V once per KV head group)
        Apply RoPE to Q and K at pos
        Write K,V[pos] to KV RAM

    S2: Score pass (sequential)
        For each head: stream K[t] for t=0..pos; accumulate s[t] = dot(Q, K[t]) * inv_sqrt_d
        Track max per head for softmax stability

    S3: Softmax and value pass (sequential)
        Re‑scan t=0..pos: a[t] = exp(s[t]−max)/sum; accumulate O_head += a[t]*V[t]

    S4: Output projection + residual
        O = concat_heads(O_head) · Wo
        x ← x + O

    S5: FFN (pre‑norm + SwiGLU + projection) + residual
        ŷ = RMSNorm_ffn(x)
        h1 = SiLU(ŷ·W1); h3 = ŷ·W3; z = (h1 ⊙ h3) · W2
        x ← x + z


-------------------------------------


3. KV-cache write/read timing
Make sure the K/V computed at Cycle2 are actually the ones written in Cycle4:

    In Cycle4 you do writeSequencer isC4 pos (getK dataSig kv, getV dataSig kv).
    dataSig at Cycle4 must contain the keys/values that were computed in Cycle2 for this same layer/pos. If the pipeline register that holds IntermediateData is only updated on certain stage boundaries, it’s easy to end up writing zeros (initial state) into RAM. Quick check:
    Put an assertion or a dump around Cycle4 that prints a few K/V elements being written (for pos>0) and compare to the values you produced in Cycle2. If they’re zero at write time, you’re not latching the Cycle2 results into dataSig before Cycle4 uses them.
    Symptom of broken cache writes: generations look like shallow “bag of words,” because attention can only see the current token (you bypass t==pos correctly) but not previous timesteps (RAM returns zeros).

Actionable checklist to get to bit-for-bit parity

    Probe KV writes (Cycle4) and reads (Cycle3) as described to ensure the cache is populated with your Cycle2 K/V.
    If your checkpoint comes from the “skip RoPE tables” format, compute RoPE on-the-fly.

--------------------------------- Bug fixing

[L-1 P1] token_embedding: 0.00937854 0.00358759 -0.00300581 0.00509678 ...
[L0 P1] x_input: 0.00937854 0.00358759 -0.00300581 0.00509678 ...
[L0 P1] (a) xHat: 0.33962446 0.12970327 -0.12086352 0.15895793 ...
[L0 P1] Wk[layer]: 0.02185008 0.01131227 0.11289575 0.04090091 ...
[L0 P1] Wv[layer]: -0.01294049 -0.02009439 0.01286488 -0.03823433 ...
[L0 P1] Wq[layer]: 0.03963275 -0.06310233 0.07051089 0.01031099 ...
[L0 P1] q_before_rope: -1.93851101 1.00818145 -1.12214458 1.37985384 ...
[L0 P1] k_before_rope: -0.65414828 2.04251361 0.12615244 2.18476534 ...
[L0 P1] v: -0.05412852 0.07323970 0.23944359 0.12235010 ...
[L0 P1] q_after_rope: -1.89573729 -1.08647799 -1.74066472 0.36509132 ...
[L0 P1] k_after_rope: -2.07215381 0.55312794 -1.27796888 1.77648795 ...
<---- All correct above this
[L0 P1] (b) concat_head: -0.02461964 0.04820975 0.08826763 -0.03566965 ...
<---- All wrong below this
[L0 P1] (c) WO@head_concat: 0.02253111 -0.01494446 0.01645944 -0.00483688 ...
[L0 P1] (d) x_after_attn: 0.03190965 -0.01135687 0.01345363 0.00025989 ...
[L0 P1] (e) xHat_ffn: 0.97812891 -0.33293912 0.40313324 0.00789774 ...
[L0 P1] (f) W1*xHat_ffn: -0.41960114 1.06537604 0.00839459 -0.18624441 ...
[L0 P1] (g) W3*xHat_ffn: -0.12423400 0.02927443 0.07437226 -0.23835002 ...
[L0 P1] (h) ffn_core: 0.02067489 0.02319524 0.00031347 0.02013472 ...
[L0 P1] (i) x_after_ffn: -0.05270804 0.01073516 -0.14348659 -0.09891579 ...